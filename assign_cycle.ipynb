{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSTALL ALL OF THESE PACKAGES USING ANACONDA OR PIP FIRS\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import networkx as nx\n",
    "import csv\n",
    "import argparse \n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processExt(path, keyname):\n",
    "    '''\n",
    "    Helper function that proceses the origal extract to only keep the internal matches in 1 source only\n",
    "    @param path: str, path to where the extract is. Best to be in the same directory as this file.\n",
    "    @keyname: str, name of the record source to restrict the path into.\n",
    "    '''\n",
    "    ext = pd.read_csv(path)\n",
    "    ext[['RECORD_ID1','RECORD_ID2']] = ext[['RECORD_ID1','RECORD_ID2']].astype(int)\n",
    "    ext = ext.loc[(ext.RECORD2_SOURCE == keyname) & (ext.RECORD1_SOURCE == ext.RECORD2_SOURCE) & (ext.DECISION =='M')][['RECORD_ID1','RECORD_ID2','MATCH_PROBABILITY']].drop_duplicates()\n",
    "    return ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InternalDeduplicator():\n",
    "    '''\n",
    "    This class takes the extract produces by ChoiceMaker and uses NetowrkX to locate \n",
    "    cycles and assign a single unique ID to each key ID in the extract (NOT THE ENTIRE SET)\n",
    "    '''\n",
    "    def __init__(self, extract, keyname:str):\n",
    "        '''\n",
    "        Initializer\n",
    "        @param extract: dataframe, should only contain only internal MATCHES from a single source ONLY\n",
    "        @param keyname: string, name of the key in the data\n",
    "        '''\n",
    "        self.ext = extract[['RECORD_ID1',  'RECORD_ID2','MATCH_PROBABILITY']] #might change to RECORD1_ID1 next iteration\n",
    "        self.keyname = keyname\n",
    "        \n",
    "    def __getCycles(self):\n",
    "        '''\n",
    "        Use NetowrkX to return the cycles.\n",
    "        '''\n",
    "        ##I. REVERSE THE EDGES' DIRECTION TO CREATE FULL GRAPH\n",
    "        #Add the back edges by reversing the direction between each pair. \n",
    "        self.ext_back = self.ext.copy()\n",
    "        # ext_back = ext[['RECORD_ID2','RECORD_ID1']]\n",
    "        self.ext_back = self.ext_back.rename(columns={'RECORD_ID2':'RECORD_ID1','RECORD_ID1':'RECORD_ID2'})\n",
    "        self.all_edges = pd.concat([self.ext,self.ext_back],axis=0)\n",
    "        self.all_edges.sort_values('RECORD_ID1', inplace=True)\n",
    "                     \n",
    "        ##Use network graph to find all the cycles\n",
    "        self.G = nx.DiGraph(self.all_edges[['RECORD_ID1','RECORD_ID2']].values.tolist())                      \n",
    "        self.dup = []\n",
    "        self.max_len = 1 #initialize with cycle of size 1 (a vertice by itself)\n",
    "\n",
    "        for cycle in nx.simple_cycles(self.G):\n",
    "            if len(cycle) > 1:\n",
    "                self.dup.append(cycle)\n",
    "                if len(cycle)>self.max_len:\n",
    "                    self.max_len = len(cycle) #keep track of the max size of cycles\n",
    "       \n",
    "        ## CREATE SET OF UNIQUE CYCLES ONLY\n",
    "        self.dup_set = list(set(frozenset(item) for item in self.dup))\n",
    "        self.dup_unique = [list(item) for item in self.dup_set]\n",
    "        \n",
    "        ## REFORMAT INTO DATAFRAME\n",
    "        self.dup_df = pd.DataFrame(self.dup_unique, columns=['id_'+str(i+1) for i in range(self.max_len)])\n",
    "        self.dup_df['num_v'] = self.max_len -  self.dup_df.isnull().sum(axis=1)\n",
    "        ##Force ID's to string \n",
    "        self.dup_df.fillna(0,inplace=True)\n",
    "        self.dup_df = self.dup_df.astype({'id_1':int, 'id_2':int, 'id_3':int, 'id_4':int,'id_5':int,'id_6':int })\n",
    "        self.dup_df['dupset'] = self.dup_df.apply(lambda x: frozenset(x[:x.num_v]),axis=1)\n",
    "        \n",
    "    def __deleteSmallCycle(self, zformat=6):\n",
    "        '''\n",
    "        Give the dataframe of all cycles, return a clean up dataframe in which \n",
    "        only the set of all postives are deleted. \n",
    "        Ex: biggest cycle of size (1,2,3,4,5). Then all sub-cycles (1,2,3),(2,3,4,5,6) \n",
    "        are DELETED if they exist\n",
    "        @param zformat: int, the length of the unique_ID to be assigned. Default 6.\n",
    "        '''\n",
    "        cy_df = self.dup_df\n",
    "        max_n = cy_df.num_v.max()\n",
    "        tbd_all = [] \n",
    "        for s in range(max_n, 2 , -1):\n",
    "            for i, r in cy_df.loc[cy_df.num_v == s,:].iterrows():\n",
    "                for c in combinations(r[:s], s-1):\n",
    "                    frozen_c = frozenset(c)\n",
    "                    #Get the index to be deleted\n",
    "                    tbd = cy_df[cy_df.dupset == frozen_c].index.tolist()\n",
    "                    tbd_all.append(tbd)\n",
    "        #                 print(c)\n",
    "\n",
    "        #             print('-------------')\n",
    "        self.tbd_all = np.unique(np.array(sum(tbd_all,[])))\n",
    "        self.clean_dup = cy_df[~cy_df.index.isin(self.tbd_all)].sort_values('num_v')  \n",
    "#         return tbd_all, cy_df[~cy_df.index.isin(tbd_all)].sort_values('num_v')  \n",
    "        self.clean_dup['temp'] = np.arange(1,self.clean_dup.shape[0]+1)\n",
    "        self.clean_dup['unique_id'] = self.clean_dup.temp.apply(lambda x: self.keyname +str(x).zfill(zformat))\n",
    "        del self.clean_dup['temp']\n",
    "        \n",
    "        \n",
    "    def getCycleReport(self):\n",
    "        '''\n",
    "        Print the reports for the cycles found, not yet deduplicated.\n",
    "        '''\n",
    "        self.__getCycles()\n",
    "        self.__deleteSmallCycle()\n",
    "        print(\"COUNT OF THE NUMBER CYCLES WITH SIZE NUM_V:\\n\")\n",
    "        print(self.clean_dup.groupby('num_v').num_v.count())\n",
    "        \n",
    "        \n",
    "    def __assignID(self):\n",
    "        '''\n",
    "        Assign a unique ID to each key in the source using the cycles produced.\n",
    "        '''\n",
    "        self.idf = pd.DataFrame(columns=[self.keyname, 'unique_id', 'num_v'])\n",
    "        index = 0\n",
    "        for _, r in self.clean_dup.iterrows():\n",
    "            for item in r.dupset:\n",
    "                self.idf.loc[index,:] = (item, r.unique_id, r.num_v)\n",
    "                self.index+=1\n",
    "        #     print(i)\n",
    "        self.idf.drop_duplicates().sort_values('unique_id', inplace=True)\n",
    "        self.idf.reset_index(inplace=True, drop=True)\n",
    "        return self.idf\n",
    "\n",
    "    def getTies(self):\n",
    "        '''\n",
    "        Get the original ID that is in 2 seprate cycles.\n",
    "        '''\n",
    "        self.dup_id = self.__assignID()\n",
    "        m = self.dup_id.groupby(self.keyname)[self.keyname].count()\n",
    "        check = m[m>1].index.tolist()\n",
    "        self.check_df = self.dup_id[self.dup_id[self.keyname].isin(check)].sort_values(self.keyname)\n",
    "        self.check_df.sort_values(self.keyname)\n",
    "        self.check_dup =  self.clean_dup.loc[self.clean_dup.unique_id.isin(self.check_df.unique_id)]\n",
    "    \n",
    "    def __lookupWeights(self, group, df):\n",
    "        '''\n",
    "        Helper function: given a group as set, and a lookup extract that contains RECORD_ID1, RECORD_ID2\n",
    "        and MATCH_PROBABILITY, return the AVERAGE weight of that group.\n",
    "        @group: a frozen set that contains all the vertices in the cycle\n",
    "        @df: dataframe, that contains all the match pairs with probabiltiies, or the CM extracts\n",
    "        '''\n",
    "        w_all = []\n",
    "        try:\n",
    "            for g in combinations(group,2):\n",
    "                ##Look  up the edge weight \n",
    "                w1 = df.loc[(df.RECORD_ID1 ==  g[0]) &  (df.RECORD_ID2 == g[1]), 'MATCH_PROBABILITY'].values\n",
    "                w2 = df.loc[(df.RECORD_ID2 ==  g[0]) &  (df.RECORD_ID1 == g[1]), 'MATCH_PROBABILITY'].values\n",
    "                w = max(set.union(set(w1),set(w2)))\n",
    "                #Add to the general weight\n",
    "        #         print(w_all)\n",
    "                w_all.append(w)\n",
    "        except:\n",
    "            print(\"ERROR, check g:\", g)\n",
    "        return np.mean(w_all) #Return the avarage of the weights\n",
    "    \n",
    "    def getTieBreaker(self):\n",
    "        '''\n",
    "        If a keu ID is assigned 2 or more unique IDs, then assign to the one with the highest avarage probability\n",
    "        '''\n",
    "        cycle_weights = pd.Series(self.check_dup.apply(lambda x: self.__lookupWeights(x.dupset,self.ext), axis=1), name='cycle_weights')\n",
    "        self.check_dup = pd.concat([self.check_dup, cycle_weights], axis=1)\n",
    "        ##Merge to get the final list\n",
    "        prefinal = pd.merge(left=self.check_df, left_on='unique_id'\n",
    "        ,right=self.check_dup[['unique_id','cycle_weights']], right_on='unique_id'\n",
    "        ,how='left').drop_duplicates()\n",
    "        \n",
    "        final = prefinal.groupby(self.keyname).max()[['cycle_weights','unique_id','num_v']]\n",
    "        self.final = final.reset_index()\n",
    "        \n",
    "        \n",
    "    def getFinalUniqueID(self):\n",
    "        '''\n",
    "        Implement tie-breaker so all origal ID has only 1 new assign ID.\n",
    "        '''\n",
    "        self.key_del = self.final[self.keyname].unique()\n",
    "        self.dup_id.drop(self.dup_id[self.dup_id[self.keyname].isin(self.key_del)].index, inplace=True)\n",
    "        self.final_id = pd.concat([self.dup_id, self.final[[self.keyname,'unique_id','num_v']]]).drop_duplicates().reset_index(drop=True)\n",
    "        return self.final_id\n",
    "    \n",
    "    \n",
    "    def runAll(self):\n",
    "        '''\n",
    "        Wrapper function that runs from beginning to end after Initializer.\n",
    "        For convenience\n",
    "        '''\n",
    "        self.__getCycles()\n",
    "        self.__deleteSmallCycle()\n",
    "        self.getTies()\n",
    "        self.getTieBreaker()\n",
    "        final = self.getFinalUniqueID()\n",
    "        return final \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST 1 FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process the extracts to leave only internal matches among JCATS \n",
    "ext = pd.read_csv(\"extract_08751_2020_edited.csv\")\n",
    "ext[['RECORD_ID1','RECORD_ID2']] = ext[['RECORD_ID1','RECORD_ID2']].astype(int)\n",
    "ext = ext.loc[(ext.RECORD2_SOURCE == 'JCATS') & (ext.RECORD1_SOURCE == ext.RECORD2_SOURCE) & (ext.DECISION =='M')][['RECORD_ID1','RECORD_ID2','MATCH_PROBABILITY']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup = InternalDeduplicator(ext, 'JCAT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dedup.getCycles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COUNT OF THE NUMBER CYCLES WITH SIZE NUM_V:\n",
      "\n",
      "num_v\n",
      "2    2283\n",
      "3     284\n",
      "4      22\n",
      "5       1\n",
      "6       1\n",
      "Name: num_v, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dedup.getCycleReport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup.getTies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup.getTieBreaker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JCAT</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>num_v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148529586</td>\n",
       "      <td>JCAT00001</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>148551338</td>\n",
       "      <td>JCAT00003</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>148536192</td>\n",
       "      <td>JCAT00004</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148579025</td>\n",
       "      <td>JCAT00005</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>148584383</td>\n",
       "      <td>JCAT00005</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3388</th>\n",
       "      <td>148538665</td>\n",
       "      <td>JCAT02312</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3389</th>\n",
       "      <td>148549306</td>\n",
       "      <td>JCAT01632</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3390</th>\n",
       "      <td>148564668</td>\n",
       "      <td>JCAT01785</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3391</th>\n",
       "      <td>148566723</td>\n",
       "      <td>JCAT02314</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3392</th>\n",
       "      <td>148583050</td>\n",
       "      <td>JCAT02029</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3393 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           JCAT  unique_id num_v\n",
       "0     148529586  JCAT00001     2\n",
       "1     148551338  JCAT00003     2\n",
       "2     148536192  JCAT00004     2\n",
       "3     148579025  JCAT00005     2\n",
       "4     148584383  JCAT00005     2\n",
       "...         ...        ...   ...\n",
       "3388  148538665  JCAT02312     3\n",
       "3389  148549306  JCAT01632     2\n",
       "3390  148564668  JCAT01785     2\n",
       "3391  148566723  JCAT02314     3\n",
       "3392  148583050  JCAT02029     2\n",
       "\n",
       "[3393 rows x 3 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dedup.getFinalUniqueID()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup2 = InternalDeduplicator(ext, 'JCAT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = dedup2.runAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert res.JCAT.nunique() == res.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  TEST 2: IMPLEMENT USING COMMAND LINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i \"assign_cycle.py\" -ext extract_08751_2020_edited.csv -key JCATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_csv(\"JCATS_internal_dedup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFY THAT THE NUMBER OF UNIQUE RECORD_IDS IN THE EXTRACT ARE PRESERVED IN THE FINAL ASSIGNMENT\n",
    "assert output.JCATS.nunique() ==  np.unique(np.concatenate([ext.RECORD_ID1.values, ext.RECORD_ID2.values])).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFY THAT EACH ID ONLY APPEARS ONCE IN THE FINAL ASSIGNMENT\n",
    "assert output.shape[0] == output.JCATS.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
